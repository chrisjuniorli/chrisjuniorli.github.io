<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="VMFormer: End-to-End Video Matting with Transformer.">
  <meta name="keywords" content="Video Matting, Transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VMFormer: End-to-End Video Matting with Transformer</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VMFormer: End-to-End Video Matting with Transformer</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chrisjuniorli.github.io/">Jiachen Li</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://vidit98.github.io/">Vidit Goel</a><sup>3</sup>,</span>
            <span class="author-block">
              Marianna Ohanyan<sup>3</sup>,
            </span>
            <span class="author-block">
              Shant Navasardyan<sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://weiyc.github.io/">Yunchao Wei</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.humphreyshi.com/">Humphrey Shi</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>SHI Lab @ University of Oregon &amp UIUC,</span>
            <span class="author-block"><sup>2</sup>Beijing Jiaotong University</span>
            <span class="author-block"><sup>3</sup>Picsart AI Research (PAIR)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://chrisjuniorli.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <video autoplay="" muted="" loop="" playsinline="" height="100%">
          <source src="static/images/video_teaser.mp4" type="video/mp4">
        </video>
      </div>
    </div>
    <h2 class="subtitle has-text-centered">
      VMFormer has two branches for feature modeling and query modeling.
    </h2>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Video matting, which is to estimate alpha mattes for each frame of input video sequences. 
            Solutions to video matting have been dominated by deep convolutional neural networks for 
            the past few years, which has become the <i>de-facto</i> standard for both academia and 
            industry. However, they still suffer from the inductive bias of CNN on local perception 
            and the lack of long-range temporal modeling. In this paper, we propose <b>VMFormer</b>: 
            a transformer-based end-to-end method that makes predictions on alpha mattes of each corresponding 
            frame from learnable queries given a video input sequence. It leverages self-attention in the 
            transformer encoder to build global integration of feature sequences and queries to learn global 
            representations through cross-attention in the transformer decoder. We further apply long-range 
            temporal modeling to all queries along with short-range temporal modeling on feature maps. 
            Each query predicts alpha matte based on feature maps of each frame in the prediction stage. 
            Experiments show that VMFormer outperforms previous CNN-based video matting methods and sets a new state-of-the-art. 
            To our best knowledge, it is the first end-to-end video matting solution built upon a vision transformer 
            with predictions on learnable queries.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Interpolating. -->
        <h3 class="title is-4">VMFormer Network</h3>
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/fig2_arxiv_v2.png" width="70%">
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            VMFormer contains two separate paths for modeling of features and queries: a) The feature
            modeling path contains a CNN-based backbone network to extract feature pyramids and a transformer encoder integrates feature sequences
            globally with short-range feature-based temporal modeling (SFTM). b) The query modeling path has a transformer decoder for queries
            to learn global representations of feature sequences and long-range query-based temporal modeling (LQTM) are built upon all queries.
            The final alpha mattes predictions based on matrix multiplication between queries and feature maps. LayerNorm, residual connection and
            repeated blocks are omitted for simplicity.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Interpolating. -->
        <h3 class="title is-4">Visualization</h3>
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img src="static/images/fig3_arxiv.png" width="70%">
          </div>
        </div>
        <div class="columns is-centered">
          Visualization of alpha matte predictions from MODNet, RVM and VMFormer under challenging frames from the composited
          test set. VMFormer shows better ability to distinguish ambiguous foreground from background regions as shown in the magnified image patches.
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Web template is from  <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
